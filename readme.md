# Heirarchical Language Generator

Heirarchical Language Generator is an architecture for generating text using a heirarchical pipeline. HLG is *not* a transformer model, it relies mainly on markov chains and embeddings.

An HLG consists of three distinct stages:
1. **Syntactic**: This stage generates the general structure of the output via symbollic tokens, for example `{subject|noun}{action|verb}{object|noun}`. This stage does not generate any meaning. This stage uses a simple variable length markov chain to generate its output. This stage may have several layers, each layer generating a "higher resolution" version of the output, for example the first layer may generate `{text greeting}{code block}{code explanation}`, the second lthen generates the noun/verb structure of the greeting, code block, and code explanation, aka "atomic" structural tokens.
2. **Semantic**: This stage generates the actual word roots to fill in the structure generated by the syntactic stage. This stage does not use a "true" markov chain, as the markov chain is only used for candidate selection. The actual descision is made by comparing the various context vectors and the candidates via an embedding. There may be several small domain specific chains that are used at this stage to fill in the syntax tokens, but these are run in series not in parallel, so it does have "layers" like the other stages. For example, there may be a chain for generating greetings, a chain for generating code blocks, and a chain for generating code explanations. Each chain can be fairly small, as they are all domain specific.
3. **Cohesion**: This stage generates the actual final text from the word roots generated by the semantic stage. This stage uses sort of a markov chain, but has some extra stuff tacked on to allow for more liberal interpretation of the previous stage. For example `{dog}{sit}{mat}` may generate `The dog sits on the mat.` or `A dog is sitting on a mat.`. This stage may have several layers, each layer running a cohesion pass on a more general scope. This is mostly useful for code generatino for example, the first layer may generate the cohesion for a single line of code, the second layer may generate the cohesion for a block of code, and the third layer may generate the cohesion for an entire function. For regular text generation, this stage may only have one layer.

Each stage is trained independently, and mat be tweaked independently. Each stage is also trained differently. More on training below.

## Usage
This library is written in c. The two main files you have to care about are `hlg.h` and `hlg.c`. `main.cpp` is mostly just an example of how to use the library and to help with development. You dont need it.

Yes this project is a visual studio solution sue me. Its only 3 files and probably a bunch of python scripts for training.